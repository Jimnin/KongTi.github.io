---
title: 머신러닝 연구원 면접 질문
date: 2023-01-16 15:30:00 GMT+0900
categories: [채용, 면접]
tags: [면접] # TAG names should always be lowercase
pin: true
---

## 1. Deep Learning Model에서 Overfitting을 막기 위한 방법

### Data의 양을 늘려주기

- 데이터의 양이 적으면 모델이 일반화가 잘 안되고 특정 패턴이나 노이즈까지 학습 된다.
- 단순히 데이터의 양을 늘린다면 위와 같은 문제를 해결할 수 있다.
- 데이터의 양을 늘리기 힘들 땐, 데이터 증강 기법을 통해 어느정도 완화할 수 있다.
- 텍스트는 번역 후, 재번역의 과정으로 데이터를 증강할 수 있는데, 역번역(Back Translation)이라 한다.

### Model을 단순화한다

- Model이 너무 깊어지면 작은 것까지 Fitting하게 되므로, Model의 은닉층을 줄여주는 것으로<br/>어느정도 해결할 수 있다.

### 가중치 규제 적용

- Model을 수정하지 않을 경우, 복잡한 모델이 Overfitting 될 가능성이 높다.
- 때문에, 가중치에 Regularizion을 적용하여 Model에 Weight Value의 분포를 균일하게 만든다.
- (L1, L2관련 내용추가)

### Dropout

<img src="https://oi.readthedocs.io/en/latest/_images/dropout.png" title="Dropout Image"/>
- 임의로 다음 뉴런으로 전달되는 Weight를 죽이는 것이다.
- 보통은 Model 학습시에는 사용하지만, Predict에선 사용하지 않는다.
- 특정 뉴런이나 특정 데이터 조합에 편향 되는 것을 방지한다.

## 2. Batch Normalization이란?

- [혁펜하임 강의](https://youtu.be/m61OSJfxL0U)
- (관련 내용추가)

## 3. Learning Rate에 따른 학습 시간

<img src="https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png" title="lr">
- Learning Rate이 크다면, 학습 속도가 빠르며 Local Minima에 빠질 확률도 줄어든다.
- 다만, 너무 큰 경우 오히려 Loss가 커지는 Gradient Exploding(기울기 폭발)이 발생할 수 있다.
- Learning Rate이 작으면, 학습 속도가 느리고 Local Minima에 잘 빠진다.
- 때문에 Local Minima는 지나치면서 Global Minima에 도달할 수 있는 Learning Rate 설정이 필요하다.

## 4. Optimizer에 대한 간단한 설명

<img src="https://velog.velcdn.com/images/chang0517/post/f955570a-69fb-4c2a-9f19-f9c7264599ca/image.png" title="optimizer">
- Model은 Loss Function에서 Minimum을 찾게 학습한다.
- 여기서 Minimum을 찾아가는 알고리즘(방법)을 Optimizer라고 한다.
- 관성을 계산하여 Minimum을 찾는 방향으로 가속하여 학습하는 방법과<span style="color: blue">(Momentum)</span>,<br/>과거 Gradient를 확인하면서 Learning Rate를 조절하면서 학습하는 방법이 있다<span style="color: red">(StepSize)</span>.

## 5. Transformer Model과 Attention에 대해서 설명

- (추가)

## 6. NoSQL에 해당하는 DB와 장단점을 설명

- (추가)
