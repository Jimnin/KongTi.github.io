---
title: 머신러닝 연구원 면접 질문
date: 2023-01-16 15:30:00 GMT+0900
categories: [채용, 면접]
tags: [면접] # TAG names should always be lowercase
math: true
pin: true
---

## 1. Deep Learning Model에서 Overfitting을 막기 위한 방법

### Data의 양을 늘려주기

- 데이터의 양이 적으면 모델이 일반화가 잘 안되고 특정 패턴이나 노이즈까지 학습 된다.
- 단순히 데이터의 양을 늘린다면 위와 같은 문제를 해결할 수 있다.
- 데이터의 양을 늘리기 힘들 땐, 데이터 증강 기법을 통해 어느정도 완화할 수 있다.
- 텍스트는 번역 후, 재번역의 과정으로 데이터를 증강할 수 있는데, 역번역(Back Translation)이라 한다.

### Model을 단순화한다

- Model이 너무 깊어지면 작은 것까지 Fitting하게 되므로, Model의 은닉층을 줄여주는 것으로<br/>어느정도 해결할 수 있다.

### 가중치 규제 적용

- Model을 수정하지 않을 경우, 복잡한 모델이 Overfitting 될 가능성이 높다.
- 때문에, 가중치에 Regularizion을 적용하여 Model에 Weight Value의 분포를 균일하게 만든다.
- (L1, L2관련 내용추가)

### Dropout

<img src="https://oi.readthedocs.io/en/latest/_images/dropout.png" title="Dropout Image"/>

- 임의로 다음 뉴런으로 전달되는 Weight를 죽이는 것이다.
- 보통은 Model 학습시에는 사용하지만, Predict에선 사용하지 않는다.
- 특정 뉴런이나 특정 데이터 조합에 편향 되는 것을 방지한다.

## 2. Batch Normalization이란?

- [혁펜하임 강의](https://youtu.be/m61OSJfxL0U)
- (관련 내용추가)

## 3. Learning Rate에 따른 학습 시간

<img src="https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png" title="lr">

- Learning Rate이 크다면, 학습 속도가 빠르며 Local Minima에 빠질 확률도 줄어든다.
- 다만, 너무 큰 경우 오히려 Loss가 커지는 Gradient Exploding(기울기 폭발)이 발생할 수 있다.
- Learning Rate이 작으면, 학습 속도가 느리고 Local Minima에 잘 빠진다.
- 때문에 Local Minima는 지나치면서 Global Minima에 도달할 수 있는 Learning Rate 설정이 필요하다.

## 4. Optimizer에 대한 간단한 설명

<img src="https://velog.velcdn.com/images/chang0517/post/f955570a-69fb-4c2a-9f19-f9c7264599ca/image.png" title="optimizer">

- Model은 Loss Function에서 Minimum을 찾게 학습한다.
- 여기서 Minimum을 찾아가는 알고리즘(방법)을 Optimizer라고 한다.
- 관성을 계산하여 Minimum을 찾는 방향으로 가속하여 학습하는 방법과<span style="color: blue">(Momentum)</span>,<br/>과거 Gradient를 확인하면서 Learning Rate를 조절하면서 학습하는 방법이 있다<span style="color: red">(StepSize)</span>.

## 5. Transformer Model과 Attention에 대해서 설명

- (추가)

## 6. NoSQL에 해당하는 DB와 장단점을 설명

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/MongoDB_Logo.svg/2560px-MongoDB_Logo.svg.png" title="mongodb">

- NoSQL은 비관계형 데이터베이스로 스키마가 느슨하거나 없고, 분산된 데이터를 저장하는데 용이하다.
- JOIN이 불가능하므로 테이블간 관계를 정의하지 않으며, RDB보다 좀 더 많은 데이터를 저장할 수 있다.
- 또한, 스키마를 고정 시키지 않으므로 서비스 중에 데이터 테이블의 형태가 변경 돼도 큰 문제가 없다.
- 장점: Big Data처리에 용이, 분산 및 병렬 처리 비용이 저렴, 가변적 구조로 굉장히 자유로움 (장점이자 단점)
- 단점: 데이터 손실 가능성이 있음, 데이터의 일관성을 보장하지 않음 (가변적 구조 때문에)

## 7. 데이터 EDA시에 사용한 라이브러리와 방법론 (탐색적 데이터 분석)

- EDA는 데이터를 분석하고 결과를 내는 과정에 있어서 지속적으로 해당 데이터에 대한<br/>'탐색과 이해'를 기본으로 가져야 한다는 것을 의미
- 시각화 (그래프,그림) vs 비시각화 (평균,분산) 방법론이 존재

### 시각화
- Time에 따른 데이터의 변화나 데이터의 분포를 그래프를 통해 파악하는 것
- 또는, 범주형 데이터의 분포를 산점도로 이해할 수 있다.
- Python에서 Matplotlib (Seaborn)을 이용하여 데이터의 시각화를 구현할 수 있다.

### 비시각화
- 데이터의 평균이나 분산을 통해 형태를 파악하는 것
- 데이터의 표준화 (Standardization)를 구현하려는 경우, 우선적으로 진행하는 작업
- 최소-최대값을 파악하여 데이터의 스케일을 조정할 때도 해당된다.
- 다변량 데이터에 대해서는 상관관계를 파악하거나 Correlation Matrix를 구성할 때 필요하다.
- 면접에 나왔던 이상치 분석도 이 범주에 들 것으로 예상된다.
- 주로, Python의 Pandas를 이용하거나 Matrix의 경우 Seaborn을 이용한다.

## 8. 데이터간의 상관관계 분석에 대해서 설명

- (추가)

## 9. 데이터의 이상치를 처리하기 위한 방법론

### 정규분포

<img src="https://sixsigmastudyguide.com/wp-content/uploads/2020/04/s13-2.png" title="normal">

- 데이터가 정규분포를 이룬다면, 표준편차를 기준으로 이상치를 처리할 수 있다.
- $${\pm}1{\sigma}$$는 데이터의 68%, $${\pm}2{\sigma}$$는 95%, $${\pm}3{\sigma}$$ 99.7%다.
- 보통 $$3{\sigma}$$를 채택하고 '3시그마 법칙'이라고 부르기도 한다.

$$
\begin{aligned}
  & Z=\frac{x-\mu}{\sigma}
\end{aligned}
$$

- Z-score를 통해 데이터를 파악할 수 있다.
- 예를 들어서 $$x$$가 평균에 대해서 3$$\sigma$$만큼 떨어져 있다면,<br/>$$x=\mu\pm3\sigma$$로 정의되고 Z-score는 $$\pm3$$이 된다.
- 나머지 데이터는 이상치로 간주하고, 최소-최대값으로 고정하거나 제거한다.

### IQR

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/1200px-Boxplot_vs_PDF.svg.png" title="iqr">

- 정규분포가 아니라면 IQR (Inter Quantile Range)를 사용한다.
- 데이터의 분포를 사분위로 나눠 이상치를 판단한다.
- 데이터의 하위 25%를 Q1, 상위 25%를 Q3로 정의하고, $IQR=Q3-Q1$으로 정한다.
- 때문에 IQR은 데이터의 50%를 포함한다.
- $${Q1}-{\alpha}{\times}{IQR}$$ 에서 $${Q3}+{\alpha}{\times}{IQR}$$ 까지 정상 범위로 생각한다.
- 보통 $$\alpha=1.5$$를 선택하며 (99.3%), $$\alpha=3$$으로 처리하기도 한다.

### DBScan

<img src="https://miro.medium.com/max/600/0*7Kanp82Wko_FMz0g.webp" title="dbscan">

- 데이터의 밀도를 기반으로 범주형 데이터에 처리할 수 있는 방법론
- $${eps}$$ 의 반지름을 가지는 원을 만들고 갯수 (밀도)로 클러스터링
- 이때 이웃하는 데이터가 $${MinPts}$$ 개 이상이면 Core Point이며 (주로 중앙),<br/>$${MinPts}$$ 개 미만이면 Border Point로 지정할 수 있다.
- 그 외에 데이터는 이상치로 간주하고 처리할 수 있다.
