---
title: 머신러닝 연구원 면접 질문
date: 2023-01-16 15:30:00 GMT+0900
categories: [채용, 면접]
tags: [면접] # TAG names should always be lowercase
math: true
---

## 1. Deep Learning Model에서 Overfitting을 막기 위한 방법

### <center>Data의 양을 늘려주기</center>

- 데이터의 양이 적으면 모델이 일반화가 잘 안되고 특정 패턴이나 노이즈까지 학습 된다.
- <strong>단순히 데이터의 양을 늘린다면 위와 같은 문제를 해결할 수 있다.</strong>
- 데이터의 양을 늘리기 힘들 땐, 데이터 증강 기법을 통해 어느정도 완화할 수 있다.
- 텍스트는 번역 후, 재번역의 과정으로 데이터를 증강할 수 있는데, 역번역(Back Translation)이라 한다.

### <center>Model을 단순화한다</center>

- <strong>Model이 너무 깊어지면 작은 것까지 Fitting</strong>하게 되므로, Model의 은닉층을 줄여주는 것으로<br/>어느정도 해결할 수 있다.

### <center>가중치 규제 적용</center>

- Model을 수정하지 않을 경우, 복잡한 모델이 Overfitting 될 가능성이 높다.
- 때문에, 가중치에 Regularizion을 적용하여 Model에 Weight Value의 분포를 균일하게 만든다.
- (L1, L2관련 내용추가)

### <center>Dropout</center>

<img src="https://oi.readthedocs.io/en/latest/_images/dropout.png" title="Dropout Image"/>

- <strong>임의로 다음 뉴런으로 전달되는 Weight를 죽이는 것이다.</strong>
- 보통은 Model 학습시에는 사용하지만, Predict에선 사용하지 않는다.
- 특정 뉴런이나 특정 데이터 조합에 편향 되는 것을 방지한다.

## 2. Batch Normalization이란?

- <strong>말 그대로 Batch의 Data를 Normalization(정규화) 하는 것이다.</strong>
- 다만, 설명상 Normalization(정규화)보단 Standardization(표준화)에 가까워 보인다.

---

<img src="https://static.javatpoint.com/tutorial/machine-learning/images/epoch-in-machine-learning2.png" title="Epoch Image" >
- Model의 Weight 업데이트는 Loss(Cost)를 낮추는 쪽으로 업데이트 하게 되는데, 이때 업데이트 주기(?)를 Batch의 크기와 Epoch으로 조절할 수 있다.
- 위 그림에서 총 데이터가 1,000개일 때, Batch Size에 따라서 Epoch당 Iterations 수가 변화한다.
- 총 데이터의 수가 $$n$$, Batch Size는 $$BS$$, Iterations이 $$I$$라고 하면 1 Epoch당 Iterations는<br/>
$$I=\frac{n}{BS}$$<br/>
라고 정의되며, 이때 Iteration이 될 때마다 Weight가 업데이트 된다 (보통 Batch의 평균으로).

<img src="https://t1.daumcdn.net/cfile/tistory/996A6C395D0E18BB21" title="GD Image1">
### <strong>$$n=BS$$가 성립한다면??</strong><br/>Batch GD이며 전체 데이터를 보고 업데이트(개인적으로는 Normal GD라고 한다)

- 데이터 전체를 보고 업데이트 하기 때문에 Normalization이 큰 의미가 없는 것으로 예상된다.
- <span style="color: blue">장점: Global Minimum으로 안정적으로 수렴 가능, GPU 병렬 처리 용이</span>
- <span style="color: red">단점: 학습이 길다, 메모리에 데이터를 올리기 빡셈,<br/>Local Minimum에 빠지면 Shooting이 거의 발생하지 않으므로 갇혀버림</span>

### <strong>$$BS=1$$가 성립한다면??</strong><br/>Stochastic Gradient Descent(SGD)이며 데이터 하나를 보고 업데이트

- 데이터 하나를 보고 바로 업데이트하므로 (즉, Step이 빠름) 수렴이 빠르게 된다.
- <span style="color: blue">장점: Local Minimum을 Shooting으로 빠져나옴, 수렴속도 빠름</span>
- <span style="color: red">단점: GPU 병렬 처리 불가, Global Minimum 근처에서 튀는 경우가 있음<br/>(데이터 하나가 모집단을 대표할 수 없으므로)</span>

<img src="https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9987E53A5D0E1B7C2E" title="GD Image2">

### <strong>$$1<<BS<<n$$가 성립한다면??</strong><br/>Mini-Batch SGD(MB SGD)이며 데이터의 BS개를 보고 업데이트

- Normal GD와 SGD의 절충안 느낌, 임의의 BS개만큼 데이터를 보고 업데이트 한다.
- <span style="color: blue">장점: Local Minimum 일부 해결, GPU 병렬 처리, 메모리 문제 해결</span>
- <span style="color: red">단점: 적절한 Size 결정, 낮은 확률로 Mini-Batch에 데이터 편향 발생</span>

---

- 본론으로 돌아가서 SGD 형태에서 Normalization을 많이하는데 Batch마다 편향을 줄여주기 위해서 시도
  - 시도는 이런데 최신 논문에선 유의미하지 않다고 결과도 있고...무튼 학습할 땐 좋다.
  - 딥러닝 블로거(?)들은 Loss Function Field를 Smoothing하는 효과 때문이라고도 예상한다.
- <strong>간단하게 Batch의 데이터가 Weight Sum한 결과의 평균/분산을 구한 후에 정규분포 형태로 표준화한다.</strong><br/>(분포 자체가 정규분포가 아니라도 크게 상관 없다. 편의상 정규분포 형태라고 지칭)

<img src="https://blog.kakaocdn.net/dn/kYezx/btrn7QUs41g/CJFt30ZxNt8BQ1yPBskqAK/img.png" title="Normal">

1. 예를 들어, 1개의 Mini-Batch안에 32개의 데이터가 있다면 <strong>노드당 32개의 Weight Sum한 결과가 나온다.</strong>
2. 그걸 Normalization해서 Activation Function으로 넘겨준다.
3. Activation Function을 통과한 결과로 다시 Weight Sum 실시
4. (2-3과정 반복)
5. Real Value와의 Error를 이용하여 Batch의 데이터 평균으로 업데이트 <strong>(업데이트 될 Value의 평균)</strong>
   - 여기서 위 식의 $$\gamma$$, $$\beta$$도 학습될 값으로 둔다.
   - Stable한 Value가 아닌 이유는 <strong>근처로 데이터가 뿌려지는데</strong> 이러면 Activation Function에서 <strong>선형적으로 근사된 부분만 지나갈 확률이 높아지고</strong> 이러면 DL 모델 전체의 파국으로 이어진다....

<img src="https://thumb.tv.zumst.com/s/zum.tv-zum-cf/kbs/2018/09/19/646a15f658821c19a687e92ed90c08e8.jpg" title="pakuk">

6. Test시엔 2에서 구한 평균의 평균으로 Normalization해서 Test를 한다 (데이터가 1개이기 때문)
   - 평균의 평균을 구할 때 Moving/Exponential 평균이 있다.
   - Moving Average는 보통 구하는 평균, Exponential은 최근의 평균값에 좀더 Weight를 준다.

- <span style="color: red">$$n$$이 충분히 크고 이에 따라서 $$BS$$도 충분히 클 때, Batch Normalization이 유의미한 결과를 보인다.</span>
- [혁펜하임 강의](https://youtu.be/m61OSJfxL0U), [꾸준희님 블로그](https://eehoeskrap.tistory.com/430)

## 3. Learning Rate에 따른 학습 시간

<img src="https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png" title="lr">

- <strong>Learning Rate이 크다면,</strong> 학습 속도가 빠르며 (Step이 크므로) Local Minima에 빠질 확률도 줄어든다.
- 다만, 너무 큰 경우 오히려 Loss가 커지는 <strong>Gradient Exploding(기울기 폭발)</strong>이 발생할 수 있다.
- <strong>Learning Rate이 작으면,</strong> 학습 속도가 느리고 (Step이 작으므로) <strong>Local Minima에 잘 빠진다.</strong>
- 때문에 Local Minima는 지나치면서 Global Minima에 도달할 수 있는 Learning Rate 설정이 필요하다.

## 4. Optimizer에 대한 간단한 설명

<img src="https://velog.velcdn.com/images/chang0517/post/f955570a-69fb-4c2a-9f19-f9c7264599ca/image.png" title="optimizer">

- Model은 Loss Function에서 Minimum을 찾게 학습한다.
- 여기서 Minimum을 찾아가는 알고리즘(방법)을 Optimizer라고 한다.
- 관성을 계산하여 Minimum을 찾는 방향으로 가속하여 학습하는 방법과<span style="color: blue">(Momentum)</span>,<br/>과거 Gradient를 확인하면서 Learning Rate를 조절하면서 학습하는 방법이 있다<span style="color: red">(StepSize)</span>.

## 5. Transformer Model과 Attention에 대해서 설명

- (추가)

## 6. NoSQL에 해당하는 DB와 장단점을 설명

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/MongoDB_Logo.svg/2560px-MongoDB_Logo.svg.png" title="mongodb">

- NoSQL은 비관계형 데이터베이스로 <strong>스키마가 느슨하거나 없고, 분산된 데이터를 저장하는데 용이</strong>하다.
- JOIN이 불가능하므로 테이블간 관계를 정의하지 않으며, <strong>RDB보다 좀 더 많은 데이터를 저장할 수 있다.</strong>
- 또한, 스키마를 고정 시키지 않으므로 서비스 중에 데이터 테이블의 형태가 변경 돼도 큰 문제가 없다.
- <span style="color: blue">장점: Big Data처리 용이, 분산/병렬 처리 비용이 저렴, 가변적 구조로 굉장히 자유로움 (장점이자 단점)</span>
- <span style="color: red">단점: 데이터 손실 가능성이 있음, 데이터의 일관성을 보장하지 않음 (가변적 구조 때문에)</span>

## 7. 데이터 EDA시에 사용한 라이브러리와 방법론 (탐색적 데이터 분석)

- <strong>EDA는 데이터를 분석하고 결과를 내는 과정에 있어서 지속적으로 해당 데이터에 대한<br/>'탐색과 이해'를 기본으로 가져야 한다는 것을 의미</strong>
- 시각화 (그래프,그림) vs 비시각화 (평균,분산) 방법론이 존재

### <center>시각화</center>

- Time에 따른 데이터의 변화나 데이터의 분포를 <strong>그래프를 통해 파악</strong>하는 것
- 또는, 범주형 데이터의 분포를 산점도로 이해할 수 있다.
- Python에서 Matplotlib (Seaborn)을 이용하여 데이터의 시각화를 구현할 수 있다.

### <center>비시각화</center>

- <strong>데이터의 평균이나 분산을 통해 형태를 파악하는 것</strong>
- 데이터의 표준화 (Standardization)를 구현하려는 경우, 우선적으로 진행하는 작업
- 최소-최대값을 파악하여 데이터의 스케일을 조정할 때도 해당된다.
- 다변량 데이터에 대해서는 상관관계를 파악하거나 Correlation Matrix를 구성할 때 필요하다.
- 면접에 나왔던 이상치 분석도 이 범주에 들 것으로 예상된다.
- 주로, Python의 Pandas를 이용하거나 Matrix의 경우 Seaborn을 이용한다.

## 8. 데이터간의 상관관계 분석에 대해서 설명

- (추가)

## 9. 데이터의 이상치를 처리하기 위한 방법론

### <center>정규분포</center>

<img src="https://sixsigmastudyguide.com/wp-content/uploads/2020/04/s13-2.png" title="normal">

- 데이터가 정규분포를 이룬다면, 표준편차를 기준으로 이상치를 처리할 수 있다.
- $${\pm}1{\sigma}$$는 데이터의 68%, $${\pm}2{\sigma}$$는 95%, $${\pm}3{\sigma}$$는 99.7%가 해당된다.
- <strong>보통 $$3{\sigma}$$를 채택하고 '3시그마 법칙'이라고 부르기도 한다.</strong>

$$
\begin{aligned}
  & Z=\frac{x-\mu}{\sigma}
\end{aligned}
$$

- Z-score를 통해 데이터를 파악할 수 있다.
- 예를 들어서 $$x$$가 평균에 대해서 3$$\sigma$$만큼 떨어져 있다면,<br/>$$x=\mu\pm3\sigma$$로 정의되고 Z-score는 $$\pm3$$이 된다.
- 나머지 데이터는 이상치로 간주하고, 최소-최대값으로 고정하거나 제거한다.

### <center>IQR</center>

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Boxplot_vs_PDF.svg/1200px-Boxplot_vs_PDF.svg.png" title="iqr">

- 정규분포가 아니라면 IQR (Inter Quantile Range)를 사용한다.
- <strong>데이터의 분포를 사분위로 나눠 이상치를 판단한다.</strong>
- 데이터의 하위 25%를 Q1, 상위 25%를 Q3로 정의하고, $IQR=Q3-Q1$으로 정한다.
- 때문에 IQR은 데이터의 50%를 포함한다.
- $${Q1}-{\alpha}{\times}{IQR}$$ 에서 $${Q3}+{\alpha}{\times}{IQR}$$ 까지 정상 범위로 생각한다.
- 보통 $$\alpha=1.5$$를 선택하며 (99.3%), $$\alpha=3$$으로 처리하기도 한다.

### <center>DBScan</center>

<img src="https://miro.medium.com/max/600/0*7Kanp82Wko_FMz0g.webp" title="dbscan">

- <strong>데이터의 밀도를 기반으로 범주형 데이터에 처리할 수 있는 방법론</strong>
- $${eps}$$ 의 반지름을 가지는 원을 만들고 갯수 (밀도)로 클러스터링
- 이때 이웃하는 데이터가 $${MinPts}$$ 개 이상이면 Core Point이며 (주로 중앙),<br/>$${MinPts}$$ 개 미만이면 Border Point로 지정할 수 있다.
- 그 외에 데이터는 이상치로 간주하고 처리할 수 있다.
